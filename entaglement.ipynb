{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (one large matrix?)\n",
    "datafiles = ['../project/dataset1.csv', '../project/dataset2.csv', '../project/dataset3.csv', '../project/dataset4.csv', '../project/dataset5.csv']\n",
    "n_samples = 50000           # Number of samples\n",
    "n_dataset = len(datafiles)  # Number of datasets\n",
    "train = 0.9   # 245000 (77.777...%)\n",
    "test = 0.1    # 35000 (10%)\n",
    "# val = 1-train # 70000 (Rest)\n",
    "\n",
    "size_test = int(n_samples*n_dataset*test)+1\n",
    "size_train = int((n_samples*n_dataset-size_test)*train)+1\n",
    "# size_val = int((n_samples*n_dataset-size_test)*val)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 5)\n",
      "0.195132062642601\n",
      "Number of entangled states: 127330\n",
      "\n",
      "Number of separable states: 122670\n",
      "\n",
      "(225000, 4)\n",
      "(25000, 4)\n",
      "(225000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Split all datasets\n",
    "datasets = np.empty((int(n_samples*n_dataset),5))\n",
    "print(datasets.shape)\n",
    "for i,datafile in enumerate(datafiles):\n",
    "  dataset = np.loadtxt(datafile, delimiter=',')\n",
    "  datasets[int(i*n_samples):int(n_samples*(i+1)),:] = dataset\n",
    "print(datasets[0,0])\n",
    "# Split into input and output\n",
    "Cr = datasets[:,0:4]   # Input: relative entropy of coherence\n",
    "ES = np.array([float(y) for y in datasets[:,4]])  # Output: entangled/separable\n",
    "\n",
    "# Count number of entangled and separable states\n",
    "n_entangled = len([es for es in ES if es == 1])\n",
    "print(f'Number of entangled states: {n_entangled}\\n')\n",
    "print(f'Number of separable states: {n_samples*n_dataset-n_entangled}\\n')\n",
    "# Split into train-and-validation and test\n",
    "#break datasets into train and test\n",
    "\n",
    "Cr_train, Cr_test, ES_train, ES_test = train_test_split(Cr, ES, test_size=test,\n",
    "                                                      random_state=42)\n",
    "# # Split into train and validation\n",
    "# Cr_train, Cr_val, ES_train, ES_val = train_test_split(Cr_trainval, ES_trainval, test_size=val,\n",
    "#                                                       random_state=30)\n",
    "# DEBUG\n",
    "print(np.shape(Cr_train))\n",
    "print(np.shape(Cr_test))\n",
    "print(np.shape(ES_train))\n",
    "print(np.shape(ES_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tansform data into tensors\n",
    "Cr_tensor = torch.tensor(Cr, dtype=torch.float32)\n",
    "ES_tensor = torch.tensor(ES, dtype=torch.float32)\n",
    "# Cr_train = torch.tensor(Cr_train, dtype=torch.float32)\n",
    "# Cr_test = torch.tensor(Cr_test, dtype=torch.float32)\n",
    "# ES_train = torch.tensor(ES_train, dtype=torch.float32)\n",
    "# ES_test = torch.tensor(ES_test, dtype=torch.float32)\n",
    "\n",
    "# # In case we have a gpu\n",
    "Cr_tensor = Cr_tensor.to(device)\n",
    "ES_tensor = ES_tensor.to(device)\n",
    "# Cr_train = Cr_train.to(device)\n",
    "# Cr_test = Cr_test.to(device)\n",
    "# ES_train = ES_train.to(device)\n",
    "# ES_test = ES_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "tensor_dataset = TensorDataset(Cr_tensor, ES_tensor)\n",
    "dataset = DataLoader(tensor_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class NeuralNetwork_3layer_with_a0(nn.Module):\n",
    "    def __init__(self, n_nodes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4, n_nodes)\n",
    "        self.layer2 = nn.Linear(n_nodes,4)\n",
    "        self.layer3 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      # if input x provided, describe how to produce output tensor\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.sig(self.layer2(x))\n",
    "        x = self.sig(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self, l):\n",
    "      # Initialize weights uniformly\n",
    "      if isinstance(l, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(l.weight)\n",
    "        #nn.init.uniform_(l.weight)\n",
    "        l.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, predicted, target, weights, input, offset):\n",
    "        bce_loss = self.bce_loss(predicted, target)\n",
    "        additional_loss = torch.sum(weights*input) + offset\n",
    "        if additional_loss <= 4:\n",
    "            additional_loss = 0\n",
    "        else:\n",
    "            additional_loss = 1\n",
    "\n",
    "        total_loss = bce_loss + additional_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        y = y.view(-1, 1)\n",
    "        pred = model(X)\n",
    "        weights = list(model.layer3.parameters())[0].data[0]\n",
    "        offset = list(model.layer3.parameters())[1].data[0]\n",
    "        loss = loss_fn(pred, y, weights, X, offset)\n",
    "        # loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_model(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # Reshape y to match pred's shape\n",
    "            y = y.view(-1, 1)  # Assuming y has shape (batch_size, 1)\n",
    "            pred = model(X)\n",
    "            weights = list(model.layer3.parameters())[0].data[0]\n",
    "            offset = list(model.layer3.parameters())[1].data[0]\n",
    "            test_loss += loss_fn(pred, y, weights, X, offset).item()\n",
    "            # test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 17.705277 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 38.416301 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 56.094977 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 67.231701 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 74.307088 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 80.147716 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 84.137206 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 87.084696 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 90.144132 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 92.306541 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 93.781467 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 95.059820 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 96.175828 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 97.190193 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 97.420892 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 98.200681 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 98.766093 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 99.462571 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 100.113921 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 101.038051 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 101.950441 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 102.589634 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 103.208335 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 103.745314 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 104.256331 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 104.964630 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 105.355113 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 105.936913 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 106.543622 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 107.267810 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 107.711102 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 108.321319 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 109.117324 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 109.540357 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 110.326843 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 111.133412 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 111.601576 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 112.106296 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 112.860661 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 113.691271 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 114.310153 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 115.105016 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.1%, Avg loss: 116.083548 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 116.705194 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 117.724966 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 118.089717 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 119.040827 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 119.751711 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 120.420769 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 121.100685 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork_3layer_with_a0(50).to(device)\n",
    "loss_fn = CustomLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_model(dataset, model, loss_fn, optimizer)\n",
    "    test_model(dataset, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1951, 0.2041, 0.1951, 0.0946])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "Cr = datasets[:,0:4]\n",
    "test = torch.tensor(Cr, dtype=torch.float32)\n",
    "print(test[0])\n",
    "print(ES_tensor[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model parameters of the last layer\n",
    "weights = list(model.layer3.parameters())[0].data[0]\n",
    "offset = list(model.layer3.parameters())[1].data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  8.2876, -14.6224,   9.3816,  10.7306])\n",
      "tensor(-4.0716)\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.3392%\n",
      "Accuracy for 0: 84.38901116817478%\n",
      "Accuracy for 1: 17.535537579517786%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(ES_tensor)\n",
    "predictions_of_0 = 0\n",
    "correct_predictions_of_0 = 0\n",
    "predictions_of_1 = 0\n",
    "correct_predictions_of_1 = 0\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    result = torch.sum(weights * test[i]) + offset\n",
    "    if result > 4:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "\n",
    "    # Compare with expected value\n",
    "    if result == ES_tensor[i]:\n",
    "        correct_predictions += 1\n",
    "    if ES_tensor[i] == 0:\n",
    "        predictions_of_0 += 1\n",
    "        if result == 0:\n",
    "            correct_predictions_of_0 += 1\n",
    "    if ES_tensor[i] == 1:\n",
    "        predictions_of_1 += 1\n",
    "        if result == 1:\n",
    "            correct_predictions_of_1 += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f'Accuracy: {accuracy}%')\n",
    "# Calculate accuracy for 0\n",
    "accuracy_0 = correct_predictions_of_0 / predictions_of_0 * 100\n",
    "print(f'Accuracy for 0: {accuracy_0}%')\n",
    "# Calculate accuracy for 1\n",
    "accuracy_1 = correct_predictions_of_1 / predictions_of_1 * 100\n",
    "print(f'Accuracy for 1: {accuracy_1}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
